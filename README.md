ğŸš€ Fine-Tuning GPT-2 on a Custom Text Dataset

This repository contains a complete workflow for fine-tuning the GPT-2 language model using HuggingFace Transformers, built directly from the included Jupyter Notebook:
ğŸ“˜ Foundational_model_gpt2.ipynb

The project demonstrates how to:

Prepare and tokenize custom datasets

Configure data loaders and training pipeline

Fine-tune GPT-2 using Trainer API

Save and reuse a domain-adapted model

Generate text using the fine-tuned model

ğŸ“Œ Features

âœ”ï¸ Load GPT-2 tokenizer & model
âœ”ï¸ Automatic dataset downloading (via wget)
âœ”ï¸ Custom data preparation & chunking
âœ”ï¸ Data collator for Language Modeling
âœ”ï¸ Trainer-based fine-tuning
âœ”ï¸ Model checkpoint saving
âœ”ï¸ Inference script for text generation

ğŸ“‚ Project Structure
ğŸ“¦ gpt2-fine-tuning
 â”£ ğŸ“„ Foundational_model_gpt2.ipynb
 â”£ ğŸ“ data/
 â”ƒ â”— ğŸ“„ training_data.txt   â† auto-downloaded
 â”£ ğŸ“ output/
 â”ƒ â”— ğŸ“„ fine_tuned_model/   â† saved model
 â”£ ğŸ“„ requirements.txt
 â”— ğŸ“„ README.md

ğŸ§° Dependencies
transformers
torch
wget
datasets


Install using:

pip install -r requirements.txt

ğŸ§  Workflow Overview
ğŸ”¹ 1. Load Tokenizer
from transformers import GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

ğŸ”¹ 2. Download Training Dataset

The notebook automatically downloads the dataset using:

import wget
wget.download("https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt",
              "data/training_data.txt")

ğŸ”¹ 3. Prepare Dataset

A custom function splits long text into model-friendly chunks:

def load_dataset(file_path, tokenizer, block_size=512):
    ...

ğŸ”¹ 4. Load GPT-2 Model
from transformers import GPT2LMHeadModel
model = GPT2LMHeadModel.from_pretrained("gpt2")

ğŸ”¹ 5. Data Collator

Used for next-token prediction training:

from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

ğŸ”¹ 6. Training Configuration
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./output/fine_tuned_model",
    per_device_train_batch_size=2,
    num_train_epochs=3,
    logging_steps=50,
    save_steps=200,
)

ğŸ”¹ 7. Train the Model
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=data_collator
)

trainer.train()

ğŸ”¹ 8. Save the Fine-Tuned Model
trainer.save_model("./output/fine_tuned_model")

ğŸ¤– Text Generation (Inference)

Once the model is trained, you can generate any text:

from transformers import GPT2LMHeadModel, GPT2Tokenizer

model = GPT2LMHeadModel.from_pretrained("./output/fine_tuned_model")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

prompt = "Once upon a time"
inputs = tokenizer.encode(prompt, return_tensors="pt")

outputs = model.generate(
    inputs,
    max_length=150,
    temperature=0.7,
    top_p=0.95,
)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))

ğŸ“ˆ Results

After fine-tuning:

âœ¨ The model generates more coherent text
âœ¨ Understands domain-specific patterns better
âœ¨ Produces longer and smoother sequences
âœ¨ Adapts to writing style of the dataset
